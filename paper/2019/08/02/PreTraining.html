<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Using Pre-Training Can Improve Model Robustness and Uncertainty 정리</title>
  <meta name="description" content="Using Pre-Training Can Improve Model Robustness and Uncertainty 정리  저자 : Dan Hendrycks, Kimin Lee, Mantas Mazeika  학회 : ICML 2019  날짜 : 2019.01.28 (last revi...">
  
  <meta name="author" content="Sangheon Lee">
  <meta name="copyright" content="&copy; Sangheon Lee 2019">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai-sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="Using Pre-Training Can Improve Model Robustness and Uncertainty 정리  저자 : Dan Hendrycks, Kimin Lee, Mantas Mazeika  학회 : ICML 2019  날짜 : 2019.01.28 (last revi..." />
  <meta property="og:url" content="http://localhost:4000" />
  <meta property="og:site_name" content="pod3275" />
  <meta property="og:title" content="Using Pre-Training Can Improve Model Robustness and Uncertainty 정리" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://localhost:4000/assets/logo.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Using Pre-Training Can Improve Model Robustness and Uncertainty 정리">
  <meta name="twitter:description" content="Using Pre-Training Can Improve Model Robustness and Uncertainty 정리  저자 : Dan Hendrycks, Kimin Lee, Mantas Mazeika  학회 : ICML 2019  날짜 : 2019.01.28 (last revi...">
  <meta name="twitter:image" content="http://localhost:4000/assets/logo.png">
  <meta name="twitter:url" content="http://localhost:4000">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/paper/2019/08/02/PreTraining.html">
  <link rel="alternate" type="application/rss+xml" title="pod3275" href="http://localhost:4000/feed.xml" />
</head>


  
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

  

  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="pod3275">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        
          
          <li class="nav-link"><a href="/about/">About</a>
          
        
          
        
          
        
          
        
          
          <li class="nav-link"><a href="/posts/">Posts</a>
          
        
          
        
          
          <li class="nav-link"><a href="/typography/">Typography</a>
          
        
          
          <li class="nav-link"><a href="/category/paper/">paper</a>
          
        
          
        
          
        
      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">Using Pre-Training Can Improve Model Robustness and Uncertainty 정리</h1>
      <p class="info">by <strong>Sangheon Lee</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">August 2, 2019</div>
  <div class="post-categories">
  in 
    
    <a href="/category/paper">Paper</a>
    
  
  </div>
</section>

<article class="post-content">
  <h1 id="using-pre-training-can-improve-model-robustness-and-uncertainty-정리">Using Pre-Training Can Improve Model Robustness and Uncertainty 정리</h1>
<ul>
  <li>저자 : Dan Hendrycks, Kimin Lee, Mantas Mazeika</li>
  <li>학회 : ICML 2019</li>
  <li>날짜 : 2019.01.28 (last revised 2019.06.21)</li>
  <li>인용 : 9회</li>
  <li>논문 : <a href="https://arxiv.org/pdf/1901.09960.pdf">paper</a></li>
</ul>

<h2 id="1-introduction">1. Introduction</h2>
<h3 id="1-1-pre-training">1-1. Pre-Training</h3>
<ul>
  <li>Pre-training이란 내가 원하는 task 이외의 <strong>다른 task의 데이터를 이용하여 주어진 모델을 먼저 학습하는 과정</strong>을 말함.</li>
  <li>
    <p>특히 이미지 데이터에 대한 task를 수행하는 모델의 경우, <strong>ImageNet 데이터</strong>를 이용한 pre-training을 널리 사용함.</p>
  </li>
  <li>과거의 Pre-training
    <ul>
      <li>(<a href="https://m.blog.naver.com/PostView.nhn?blogId=laonple&amp;logNo=220884698923&amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F">다음 블로그 내용</a>을 참고함.)</li>
      <li>2006년 이전에는 hidden layer가 2개 이상인 neural network (이하 딥러닝) 의 경우 학습이 제대로 이루어지지 않아, 널리 사용하지 못하였음.</li>
      <li>딥러닝의 학습을 위해 제안된 여러 기법들 중 Bengio 교수의 <strong>Greedy Layer-Wise Training</strong>.</li>
    </ul>

    <p><img src="https://user-images.githubusercontent.com/26705935/65571500-e4077980-df9f-11e9-8aaf-c4b66884571f.png" alt="image" /></p>

    <ul>
      <li>딥러닝 모델의 각 layer의 node수와 같은 hidden layer를 하나 갖는 auto-encoder 구조의 분할 모델을 unsupervised way로 학습.</li>
      <li>
        <p>학습된 여러 auto-encoder의 weight (박스 안의 부분) 들을 원래 모델에 합쳐서, 원래 모델을 전체적으로 supervised way로 학습.</p>
      </li>
      <li>이후 dropout, relu 등 딥러닝 학습을 가능하게 하는 다양한 기법들이 제안되면서 현재는 크게 사용하지 않는 기법이지만, 딥러닝 사용을 boosting하는 획기적인 기법이었음.</li>
      <li><strong>또한, Pre-train의 개념이 처음으로 제안된 아이디어임.</strong></li>
    </ul>
  </li>
  <li>
    <p>현재의 Pre-training</p>

    <p><img src="https://user-images.githubusercontent.com/26705935/65571862-22516880-dfa1-11e9-8ec8-d7a2d7ca040f.png" alt="image" />
그림: <a href="https://www.mdpi.com/2072-4292/9/7/666/pdf-vor">출처</a></p>

    <ul>
      <li>주어진 분류 모델을, 내 데이터가 아닌 다른 데이터로 학습.</li>
      <li>
        <p>모델 뒤의 fully-connected layer (dense layer, 분류 또는 개체 감지 등의 결과를 계산하는 부분) 를 내 데이터에 알맞게 바꿔서, 전체 모델을 내 데이터로 학습.</p>
      </li>
      <li>데이터의 feature를 추출하는 것은 task independent하기 때문에, <strong>데이터의 수가 많은 다른 task를 이용하여 모델 앞 단의 feature extractor 부분을 학습하자</strong> 라는 것임.</li>
    </ul>
  </li>
  <li>이렇게 널리 쓰이는 Pre-training 기법에 대해, 회의적인 의견을 제시한 논문 등장.
    <ul>
      <li>
        <p>“Rethinking ImageNet Pre-training”, He et al., 2018. <a href="https://arxiv.org/pdf/1811.08883.pdf">paper</a> (무려 ResNet 논문의 저자 Kaiming He의 논문)</p>
      </li>
      <li>ImageNet pre-trained weight를 사용하는 건 모델 수렴을 빠르게 해준다.</li>
      <li>하지만 최종 성능을 높여주거나 overfitting을 방지해주는 역할은 없다.</li>
    </ul>

    <p><img src="https://user-images.githubusercontent.com/26705935/65572222-5ed19400-dfa2-11e9-997a-997c50d30e7b.png" alt="image" /></p>

    <ul>
      <li>그래프와 같이, weight를 randomly initialized한 모델은 학습 속도는 느림.</li>
      <li>하지만 오래 학습하다보면 pre-trained weight를 사용한 모델과 성능이 비슷해짐.</li>
      <li>
        <p>특히 오른쪽 그래프처럼, 학습 데이터 수가 적어도 성능이 비슷해지는 걸 보면, generalization 측면에서도 특효가 있지 않음.</p>
      </li>
      <li>최종적으로, pre-trained weight를 사용하는 것이 필수적인 건 아니다. (not necessary)</li>
    </ul>
  </li>
  <li>본 논문에서는 위의 논문을 반박함.
    <ul>
      <li>Pre-trained weight가 성능 개선이 없을 수 있지만, 다른 방면에서 우수성이 있다.</li>
      <li>특히, <strong>모델의 robustness 및 uncertainty</strong> 를 크게 개선한다.
        <ul>
          <li>Robustness: Adversarial Robustness, Label Corruption, Class Imbalance</li>
          <li>Uncertainty: Out-of-Distribution Detection, Calibration</li>
        </ul>
      </li>
      <li>이에 대한 다양한 실험을 진행하며, pre-trained weight의 우수성을 입증함.</li>
    </ul>
  </li>
</ul>

</article>





<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
      <a href="//twitter.com/share?text=Using+Pre-Training+Can+Improve+Model+Robustness+and+Uncertainty+%EC%A0%95%EB%A6%AC&url=http%3A%2F%2Flocalhost%3A4000%2Fpaper%2F2019%2F08%2F02%2FPreTraining.html&via=SangheonLee"
        onclick="window.open(this.href, 'twitter-share', 'width=550,height=255');return false;">
        <i class="fa fa-twitter-square fa-lg"></i>
      </a>
    
    
    
    
    
    
    
  
    
    
    
      <a href="//www.facebook.com/sharer.php?t=Using+Pre-Training+Can+Improve+Model+Robustness+and+Uncertainty+%EC%A0%95%EB%A6%AC&u=http%3A%2F%2Flocalhost%3A4000%2Fpaper%2F2019%2F08%2F02%2FPreTraining.html"
        onclick="window.open(this.href, 'facebook-share', 'width=550,height=255');return false;">
        <i class="fa fa-facebook-square fa-lg"></i>
      </a>
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
      <a href="//www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fpaper%2F2019%2F08%2F02%2FPreTraining.html"
        onclick="window.open(this.href, 'linkedin-share', 'width=550,height=255');return false;">
        <i class="fa fa-linkedin-square fa-lg"></i>
      </a>
    
    
    
    
  
    
    
    
    
      <a href="//plus.google.com/share?title=Using+Pre-Training+Can+Improve+Model+Robustness+and+Uncertainty+%EC%A0%95%EB%A6%AC&url=http%3A%2F%2Flocalhost%3A4000%2Fpaper%2F2019%2F08%2F02%2FPreTraining.html"
        onclick="window.open(this.href, 'google-plus-share', 'width=550,height=255');return false;">
        <i class="fa fa-google-plus-square fa-lg"></i>
      </a>
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
      <a href="//www.pinterest.com/pin/create/button/?description=Using+Pre-Training+Can+Improve+Model+Robustness+and+Uncertainty+%EC%A0%95%EB%A6%AC&url=http%3A%2F%2Flocalhost%3A4000%2Fpaper%2F2019%2F08%2F02%2FPreTraining.html&media=http://localhost:4000/assets/header_image.jpg"
        onclick="window.open(this.href, 'pinterest-share', 'width=550,height=255');return false;">
        <i class="fa fa-pinterest-square fa-lg"></i>
      </a>
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
      <a href="//www.reddit.com/submit" onclick="window.location = '//www.reddit.com/submit?url=' + encodeURIComponent('http://localhost:4000/paper/2019/08/02/PreTraining.html') + '&title=Using Pre-Training Can Improve Model Robustness and Uncertainty 정리'; return false">
        <i class="fa fa-reddit-square fa-lg"></i>
      </a>
    
    
  
    
    
    
    
    
    
    
    
  
</section>




<section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = 'pod3275';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">pod3275</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">
        
        
          <li class="nav-link"><a href="/about/">About</a>
        
        
        
        
        
        
        
        
        
          <li class="nav-link"><a href="/posts/">Posts</a>
        
        
        
        
        
          <li class="nav-link"><a href="/typography/">Typography</a>
        
        
        
          <li class="nav-link"><a href="/category/paper/">paper</a>
        
        
        
        
        
        
      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:lawlee1@naver.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">lawlee1@naver.com</span>
          </a>
        </li>

        
          
        
          
          <li>
            <a href="https://www.facebook.com/lawlee1LSH" title="Friend me on Facebook">
              <i class="fa fa-facebook"></i>
              <span class="username">이상헌</span>
            </a>
          </li>
          
        
          
          <li>
            <a href="https://github.com/pod3275" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">pod3275</span>
            </a>
          </li>
          
        
          
          <li>
            <a href="https://www.linkedin.com/in/sangheon-lee-626401181/" title="Connect with me on LinkedIn">
              <i class="fa fa-linkedin"></i>
              <span class="username">Sangheon Lee</span>
            </a>
          </li>
          
        
          
        
          
          <li>
            <a href="https://www.youtube.com/channel/UC4QufB9MMXa3UjEfmZTXMEA" title="Subscribe on YouTube">
              <i class="fa fa-youtube"></i>
              <span class="username">칼바람 뿍뽁이</span>
            </a>
          </li>
          
        
          
          <li>
            <a href="https://www.instagram.com/sanghoney95/" title="Follow me on Instagram">
              <i class="fa fa-instagram"></i>
              <span class="username">Sanghoney95</span>
            </a>
          </li>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">pod3275의 머신 러닝 블로그
</p>
    </div>

  </div>

</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>

<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });
});

</script>




<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-145715679-1', 'auto');
  ga('send', 'pageview', {
    'page': '/paper/2019/08/02/PreTraining.html',
    'title': 'Using Pre-Training Can Improve Model Robustness and Uncertainty 정리'
  });
</script>



  </body>

</html>
