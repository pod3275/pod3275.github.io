---
layout: post
title:  "Dropout on CNN"
date:   2019-05-30 21:23:00
author: Sangheon Lee
categories: Paper
---

# Dropout on CNN
- CNN에 dropout이 적용되는 여러 변형들을 제안한 논문들을 정리.

## 1. Introduction
### 1-1. Overfitting & Generalization
- 딥러닝 모델은 데이터가 많을수록 높은 성능을 낸다는 특성을 가짐.

  ![image](https://user-images.githubusercontent.com/26705935/59272432-f8de2c80-8c90-11e9-80be-e96519681c3f.png)

  - 출처:[링크](https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063)

- 하지만 데이터가 많다고 해서 모델 성능이 항상 좋은 건 아님.
  - 모델의 복잡도(혹은 power)가 높으면 학습에서 본 데이터를 너무 따라가는 경향이 있음.
  - ***"Overfitting"***

  ![image](https://user-images.githubusercontent.com/26705935/59351118-9ea89e80-8d58-11e9-8272-93cd60955fc4.png)

  - 출처:[링크](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)

- Overfitting을 방지하자 = Generalization
- 대표적인 generalization 기법 = Dropout
  - 신경망의 overfitting을 방지하기 위한 기법.
  - 학습 과정에서 특정 node들을 p의 확률로 제거.

  ![image](https://user-images.githubusercontent.com/26705935/59435006-a5521700-8e27-11e9-881e-5b8c0e8049b7.png)

  - Dropout을 적용하면 적용하지 않은 것과 비교하여 output 값이 p배만큼 감소함.
  - 따라서, test 과정에서는 모든 weight를 사용함 + weight들에 1/p배를 곱함.
