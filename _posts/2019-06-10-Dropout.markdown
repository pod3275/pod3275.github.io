---
layout: post
title:  "Dropout on CNN"
date:   2019-05-30 21:23:00
author: Sangheon Lee
categories: Paper
---

# Dropout on CNN
- CNN에 dropout이 적용되는 여러 변형들을 제안한 논문들을 정리.

## 1. Introduction
### 1-1. Overfitting & Generalization
- 딥러닝 모델은 데이터가 많을수록 높은 성능을 낸다는 특성을 가짐.

  ![image](https://user-images.githubusercontent.com/26705935/59272432-f8de2c80-8c90-11e9-80be-e96519681c3f.png)

  - 출처:[링크](https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063)

- 하지만 데이터가 많다고 해서 모델 성능이 항상 좋은 건 아님.
  - 모델의 복잡도(혹은 power)가 높으면 학습에서 본 데이터를 너무 따라가는 경향이 있음.
  - **"Overfitting"**

  ![image](https://user-images.githubusercontent.com/26705935/59351118-9ea89e80-8d58-11e9-8272-93cd60955fc4.png)

  - 출처:[링크](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)

  - Overfitting을 방지하자 = **Generalization**

### 1-2. Dropout
- 대표적인 generalization 기법 = **Dropout**
  - 2014년 JMLR에서 처음 제안됨. [논문](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)
  - 신경망의 overfitting을 방지하기 위한 기법.
  - 학습 과정에서 특정 node들을 p의 확률로 사용하겠다 (1-p의 확률로 제거하겠다). 0<p<1.

  ![image](https://user-images.githubusercontent.com/26705935/59435006-a5521700-8e27-11e9-881e-5b8c0e8049b7.png)

  - Dropout을 적용하면 적용하지 않은 것과 비교하여 output 값이 1/p배만큼 증가함.
  - 따라서, test 과정에서는 모든 weight를 사용함 + weight들에 p배를 곱함.

- Dropout의 효과 (실험 결과)

  ![image](https://user-images.githubusercontent.com/26705935/59604294-c166f880-9146-11e9-91f5-dc121c30aeb1.png)

  - 위 뿐만 아니라 다양한 분야에서 dropout 적용을 통해 성능 개선을 뚜렷하게 보임.

- 왜 잘하는가?
  **1) Ensemble**
  - 매 번 node가 랜덤하게 제거되는데, 각각이 독립적인 model이라고 볼 수 있음.
  - 즉, dropout을 적용하여 학습된 model은 독립적인 작은 model들의 ensemble 효과를 볼 수 있다는 주장.

  **2) Avoiding co-adaptation**
  - Co-adaptation: 학습 후 네트워크 내의 각 node들이 너무 서로 비슷한 역할을 하는 것.
  - Dropout은 결과적으로 각 node가 서로 다른 것을 학습하도록 함으로써, 네트워크 전체를 utilize할 수 있게 함.

  ![image](https://user-images.githubusercontent.com/26705935/59605143-16a40980-9149-11e9-9a82-d06b304b6699.png)

  - 그림: 중간 feature들(hidden node들의 output)의 시각화.
  - Dropout을 적용했을 때 feature들이 각기 다른 모양을 갖고, 좀 더 밝은 것(높은 값)을 확인할 수 있음.

### 1-3. Dropout의 변형
#### 1) DropConnect
- 2013년 ICML [논문](http://proceedings.mlr.press/v28/wan13.pdf) (사실 dropout 논문인 2014년보다 먼저 나왔음.)
- Dropout의 조금 더 일반화된 version.
- Node 제거 --> Weight 제거.

  ![image](https://user-images.githubusercontent.com/26705935/59605607-2112d300-914a-11e9-802a-92e39b430f0c.png)

  - 출처: [링크](https://m.blog.naver.com/laonple/220827359158)
  - 학습 과정에서 특정 weight를 p의 확률로 사용 (1-p의 확률로 제거). 0<p<1.
  - Dropout과 동일하게 test 과정에서 모든 weight를 사용 + p배.

- DropConnect 성능

  ![image](https://user-images.githubusercontent.com/26705935/59605882-c037ca80-914a-11e9-90cb-30d5a70d7d2c.png)

  - Dropout보다 조금 더 좋다.

#### 2) Drop-path
- 2016년 [논문](https://arxiv.org/pdf/1605.07648.pdf)
- FractalNet이라는 모델 제안 + Drop-path 적용.
  - FractalNet: 하나의 연산을 그림과 같이 2개로 나누고, 각각의 연산에도 적용.

  ![image](https://user-images.githubusercontent.com/26705935/59606246-9b902280-914b-11e9-96d6-11cf9d24bc38.png)

- Drop-path

  ![image](https://user-images.githubusercontent.com/26705935/59920065-406c7180-9464-11e9-8bb5-ec5e1525b215.png)

  - FractalNet의 한 path (a층부터 b층까지의 connection 경로) 내의 weight을 모두 제거하는 방식의 dropout.
  - Fractal 구조인 경우에 한정되어 적용 가능.

### 1-4. CNN에서의 Dropout
- CNN에서 Dropout은 보통 pooling layer 혹은 맨 마지막 dense layer에 적용함.
  - Convolution layer에는 적용하지 않음.
  - 이유는 convolution 연산을 통해 데이터의 spatial feature를 추출하기 때문에, 노드(output) 몇 개를 지우는 것은 추출되는 정보의 손실이 큼.
  - 실제로 convolution layer에 dropout을 적용하면 성능이 떨어짐.

- 이에 따라 convolution layer에 적용할 수 있는 dropout 기반의 generalization 기법들이 제안됨.
  - **DropBlock, DropFilter, Spectral Dropout**

## 2. DropBlock
